{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-vin3ybGueeJ"
   },
   "outputs": [],
   "source": [
    "!pip3 install datasets\n",
    "!pip3 install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 0,
     "status": "ok",
     "timestamp": 1669110367278,
     "user": {
      "displayName": "Wazenmai Chen",
      "userId": "05479987124686303265"
     },
     "user_tz": -480
    },
    "id": "0KrcvdSh8Uml",
    "outputId": "2f374eb1-2c2d-4559-d884-75aeecc92caa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import DataCollatorWithPadding\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "## Code for Google Colab\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# dir = \"/content/drive/MyDrive/Colab Notebooks/Data Mining/HW2/data/\"\n",
    "dir = \"./data/\"\n",
    "split_path = dir + \"data_identification.csv\"\n",
    "emotion_path = dir + \"emotion.csv\"\n",
    "data_path = dir + \"tweets_DM.json\"\n",
    "train_path = dir + \"train.csv\"\n",
    "test_path = dir + \"test.csv\"\n",
    "ss_path = dir + \"sampleSubmission.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8911,
     "status": "ok",
     "timestamp": 1669110415564,
     "user": {
      "displayName": "Wazenmai Chen",
      "userId": "05479987124686303265"
     },
     "user_tz": -480
    },
    "id": "z-XzntipuSKh",
    "outputId": "4166fde1-679f-4132-b9db-82b28fa327c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emotion distribution (1455563, 2)\n",
      "-------------\n",
      "joy             516017         0.354514\n",
      "anticipation    248935         0.171023\n",
      "trust           205478         0.141167\n",
      "sadness         193437         0.132895\n",
      "disgust         139101         0.095565\n",
      "fear            63999          0.043969\n",
      "surprise        48729          0.033478\n",
      "anger           39867          0.027389\n",
      "\n",
      "\n",
      "\n",
      "Data split distribution (1867535, 2)\n",
      "----------------------------\n",
      "train           1455563        0.779403\n",
      "test            411972         0.220597\n"
     ]
    }
   ],
   "source": [
    "# Print the information of emotion.csv and data_identification.csv\n",
    "\n",
    "emotion = pd.read_csv(emotion_path)\n",
    "split = pd.read_csv(split_path)\n",
    "\n",
    "emotion = emotion.convert_dtypes()\n",
    "a = emotion[\"emotion\"].value_counts()\n",
    "\n",
    "print(\"Emotion distribution\", end = ' ')\n",
    "print(emotion.shape)\n",
    "print(\"-------------\")\n",
    "print(\"%-15s %-14d %2f\" % (\"joy\", a.joy, a.joy / emotion.shape[0]))\n",
    "print(\"%-15s %-14d %2f\" % (\"anticipation\", a.anticipation, a.anticipation / emotion.shape[0]))\n",
    "print(\"%-15s %-14d %2f\" % (\"trust\", a.trust, a.trust / emotion.shape[0]))\n",
    "print(\"%-15s %-14d %2f\" % (\"sadness\", a.sadness, a.sadness / emotion.shape[0]))\n",
    "print(\"%-15s %-14d %2f\" % (\"disgust\", a.disgust, a.disgust / emotion.shape[0]))\n",
    "print(\"%-15s %-14d %2f\" % (\"fear\", a.fear, a.fear / emotion.shape[0]))\n",
    "print(\"%-15s %-14d %2f\" % (\"surprise\", a.surprise, a.surprise / emotion.shape[0]))\n",
    "print(\"%-15s %-14d %2f\" % (\"anger\", a.anger, a.anger / emotion.shape[0]))\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "a = split[\"identification\"].value_counts()\n",
    "print(\"Data split distribution\", end = ' ')\n",
    "print(split.shape)\n",
    "print(\"----------------------------\")\n",
    "print(\"%-15s %-14d %2f\" % (\"train\", a.train, a.train / split.shape[0]))\n",
    "print(\"%-15s %-14d %2f\" % (\"test\", a.test, a.test / split.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 32673,
     "status": "ok",
     "timestamp": 1669110591389,
     "user": {
      "displayName": "Wazenmai Chen",
      "userId": "05479987124686303265"
     },
     "user_tz": -480
    },
    "id": "wVNpCe-5u5rU",
    "outputId": "a2f5766d-0a17-40d7-e17a-69e883a4ff02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1867535, 3)\n",
      "                        hashtags  tweet_id  \\\n",
      "0                     [Snapchat]  0x376b20   \n",
      "1  [freepress, TrumpLegacy, CNN]  0x2d5350   \n",
      "2                   [bibleverse]  0x28b412   \n",
      "\n",
      "                                                text  \n",
      "0  People who post \"add me on #Snapchat\" must be ...  \n",
      "1  @brianklaas As we see, Trump is dangerous to #...  \n",
      "2  Confident of your obedience, I write to you, k...  \n"
     ]
    }
   ],
   "source": [
    "# read data file\n",
    "f = open(data_path, \"r\")\n",
    "lines = f.readlines()\n",
    "f.close()\n",
    "\n",
    "# convert json data to dataframe\n",
    "dict_list = []\n",
    "for line in lines:\n",
    "    new_dict = json.loads(line)[\"_source\"][\"tweet\"]\n",
    "    dict_list.append(new_dict)\n",
    "big_data = pd.DataFrame(dict_list)\n",
    "\n",
    "print(big_data.shape)\n",
    "print(big_data.head(3))\n",
    "\n",
    "# save test data\n",
    "ids = split[split[\"identification\"] == \"test\"][\"tweet_id\"].tolist()\n",
    "test_data = big_data[big_data[\"tweet_id\"].isin(ids)]\n",
    "# test_data.to_csv(dir + \"test.csv\", index=False)\n",
    "# save train data\n",
    "ids = split[split[\"identification\"] == \"train\"][\"tweet_id\"].tolist()\n",
    "train_data = big_data[big_data[\"tweet_id\"].isin(ids)]\n",
    "# train_data.to_csv(dir + \"train.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NPlynZBmve1n"
   },
   "outputs": [],
   "source": [
    "# load tokenizer\n",
    "# You can pick the model you want to use\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# lower casing the text\n",
    "train_data[\"text\"] = train_data[\"text\"].str.lower()\n",
    "test_data[\"text\"] = test_data[\"text\"].str.lower()\n",
    "\n",
    "# turn words to ids [train_data]\n",
    "input_ids = []\n",
    "attention_mask = []\n",
    "\n",
    "for ind in train_data.index:\n",
    "    tokenized_text = tokenizer(train_data.loc[ind][\"text\"], truncation=True)\n",
    "    input_ids.append(tokenized_text[\"input_ids\"])\n",
    "    attention_mask.append(tokenized_text[\"attention_mask\"])\n",
    "\n",
    "train_data[\"input_ids\"] = input_ids\n",
    "train_data[\"attention_mask\"] = attention_mask\n",
    "\n",
    "# turn words to ids [test_data]\n",
    "input_ids = []\n",
    "attention_mask = []\n",
    "\n",
    "for ind in test_data.index:\n",
    "    tokenized_text = tokenizer(test_data.loc[ind][\"text\"], truncation=True)\n",
    "    input_ids.append(tokenized_text[\"input_ids\"])\n",
    "    attention_mask.append(tokenized_text[\"attention_mask\"])\n",
    "\n",
    "test_data[\"input_ids\"] = input_ids\n",
    "test_data[\"attention_mask\"] = attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tQ8pTrz3zuGv"
   },
   "outputs": [],
   "source": [
    "# Append emotion label to dataset\n",
    "\n",
    "# make hashmap [tweet_id: emotion]\n",
    "hashmap = {}\n",
    "for ind in emotion.index:\n",
    "    hashmap[emotion.loc[ind][\"tweet_id\"]] = emotion.loc[ind][\"emotion\"]\n",
    "\n",
    "# get emotion from hashmap\n",
    "def get_emotion_from_id(id):\n",
    "    return hashmap[id]\n",
    "\n",
    "# Therefore, the whole process is O(N)\n",
    "train_data[\"emotion\"] = train_data[\"tweet_id\"].apply(lambda x : get_emotion_from_id(x))\n",
    "# If we do this command, we would need to spend O(N^2) time\n",
    "# train_data[\"emotion\"] = train_data[\"tweet_id\"].apply(lambda x : emotion.loc[emotion[\"tweet_id\"] == x].emotion.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w89G3OoawVWG"
   },
   "outputs": [],
   "source": [
    "# Turn emotion to number labels\n",
    "\n",
    "emotion_map = {\n",
    "    \"joy\": 0,\n",
    "    \"anticipation\": 1,\n",
    "    \"trust\": 2,\n",
    "    \"surprise\": 3,\n",
    "    \"sadness\": 4,\n",
    "    \"fear\": 5,\n",
    "    \"disgust\": 6,\n",
    "    \"anger\": 7,\n",
    "}\n",
    "emotion_list = [\"joy\", \"anticipation\", \"trust\", \"surprise\", \"sadness\", \"fear\", \"disgust\", \"anger\"]\n",
    "train_data[\"label\"] = train_data[\"emotion\"].apply(lambda x : emotion_map[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 884,
     "status": "ok",
     "timestamp": 1669115647535,
     "user": {
      "displayName": "Wazenmai Chen",
      "userId": "05479987124686303265"
     },
     "user_tz": -480
    },
    "id": "bxUbgiyuzJxU",
    "outputId": "71313940-b340-4e8c-d32b-6cfae7f2d329"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['hashtags', 'tweet_id', 'text', 'input_ids', 'attention_mask', 'emotion', 'label', '__index_level_0__'],\n",
      "    num_rows: 1164450\n",
      "})\n",
      "Dataset({\n",
      "    features: ['hashtags', 'tweet_id', 'text', 'input_ids', 'attention_mask', 'emotion', 'label', '__index_level_0__'],\n",
      "    num_rows: 291113\n",
      "})\n",
      "Dataset({\n",
      "    features: ['hashtags', 'tweet_id', 'text', 'input_ids', 'attention_mask', '__index_level_0__'],\n",
      "    num_rows: 411972\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "total = train_data.shape[0]\n",
    "df1 = train_data.iloc[:int(total * 0.8), :]\n",
    "df2 = train_data.iloc[int(total * 0.8):, :]\n",
    "\n",
    "# prepare training dataset\n",
    "train_ds = Dataset.from_pandas(df1)\n",
    "train_ds.save_to_disk(dir + \"train_dataset\")\n",
    "print(train_ds)\n",
    "valid_ds = Dataset.from_pandas(df2)\n",
    "valid_ds.save_to_disk(dir + \"valid_dataset\")\n",
    "print(valid_ds)\n",
    "\n",
    "# prepare submission dataset\n",
    "test_ds = Dataset.from_pandas(test_data)\n",
    "test_ds.save_to_disk(dir + \"test_dataset\")\n",
    "print(test_ds)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOlVfKdlCIRMkG3WqJY9jEH",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
